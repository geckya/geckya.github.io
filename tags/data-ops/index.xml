<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>data ops on Amanda Alvarez</title><link>https://gecky.me/tags/data-ops/</link><description>Recent content in data ops on Amanda Alvarez</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 11 Dec 2022 00:00:01 +0000</lastBuildDate><atom:link href="https://gecky.me/tags/data-ops/index.xml" rel="self" type="application/rss+xml"/><item><title>Dimensions of Data Quality</title><link>https://gecky.me/posts/data-quality-dimensions/</link><pubDate>Sun, 11 Dec 2022 00:00:01 +0000</pubDate><guid>https://gecky.me/posts/data-quality-dimensions/</guid><description>Everyone wants &amp;ldquo;good&amp;rdquo; data. Almost as universal is the sense that the data you&amp;rsquo;re working with is&amp;hellip;not good. Being able to objectively measure data quality is important for ensuring downstream modeling and decision making is built on reliable data, but it can be hard to measure and report on data quality without a framework for identifying what features of the data are good/bad.
Metadata features with expectations that can be defined (and measured against!</description></item></channel></rss>